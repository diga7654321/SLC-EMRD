import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import models, transforms
from transformers import BertModel, BertTokenizer, AdamW
from PIL import Image
import json
import os
import re
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix,accuracy_score
from tqdm import tqdm
from sklearn.metrics.pairwise import cosine_similarity

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# æ–‡æœ¬ç¼–ç å™¨
bert_path = "../Model/bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained(bert_path)

# å›¾åƒé¢„å¤„ç†
image_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# ResNet-50 å›¾åƒç¼–ç å™¨ï¼ˆç§»é™¤æœ€ååˆ†ç±»å±‚ï¼‰
resnet = models.resnet50(pretrained=True)
resnet.fc = nn.Identity()
resnet = resnet.to(device)
resnet.eval()


# è®¡ç®—LLMçš„ç½®ä¿¡åº¦
def compute_llm_confidence(llm_data, label_data, alpha=1.0):
    all_llm_probs = []
    all_labels = []
    correct_predictions = 0  # ç”¨äºè®¡ç®—æ­£ç¡®é¢„æµ‹çš„æ•°é‡
    total_predictions = 0  # ç”¨äºè®¡ç®—æ€»é¢„æµ‹æ•°

    # éå†æ‰€æœ‰LLMæ•°æ®
    for item_id, llm_raw in llm_data.items():


        true_score = float(re.search(r"true:\s*([0-9\.]+)", llm_raw).group(1))
        false_score = float(re.search(r"false:\s*([0-9\.]+)", llm_raw).group(1))
        llm_score = {'true': true_score, 'false': false_score}

        # è·å–çœŸå®æ ‡ç­¾
        item_num = item_id.split('_')[1]  # æå–æ•°å­—éƒ¨åˆ†
        label = label_data.get(item_num).get("label")  # å¦‚æœæ ‡ç­¾ä¸å­˜åœ¨ï¼Œé»˜è®¤ä¸º0
        all_llm_probs.append(llm_score)
        all_labels.append(label)

        # è®¡ç®—é¢„æµ‹ç±»åˆ«
        predicted_label = 0 if true_score > false_score else 1  # å¦‚æœtrue_scoreå¤§äºfalse_scoreï¼Œé¢„æµ‹ä¸ºtrue(1)ï¼Œå¦åˆ™ä¸ºfalse(0)

        # è®¡ç®—æ­£ç¡®é¢„æµ‹çš„æ•°é‡
        if predicted_label == label:
            correct_predictions += 1
        total_predictions += 1

    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = correct_predictions / total_predictions
    print(f"LLM Accuracy: {accuracy:.4f}")


    # è®¡ç®—è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNLLï¼‰æŸå¤±
    nll = 0.0
    total = 0.0
    for prob, label in zip(all_llm_probs, all_labels):
        # æ ¹æ®çœŸå®æ ‡ç­¾é€‰æ‹© 'true' æˆ– 'false' æ¦‚ç‡
        prob_value = prob['true'] if label == 0 else prob['false']
        nll -= torch.log(torch.tensor(prob_value) + 1e-8)  # æ·»åŠ å°çš„epsiloné¿å…log(0)
        total += 1

    # æ±‚å¹³å‡NLLæŸå¤±
    nll = nll / total

    print(f"LLM Confidence: {torch.exp(-alpha * nll):.4f}")

    # æ ¹æ®NLLè®¡ç®—å…¨å±€ç½®ä¿¡åº¦ï¼šC = exp(-Î± Ã— NLL)
    return torch.exp(-alpha * nll)


def split_dataset(data_list, test_size=0.2, val_size=0.25):
    train_data, test_data = train_test_split(data_list, test_size=test_size, random_state=42)
    val_size_adjusted = val_size / (1 - test_size)
    train_data, val_data = train_test_split(train_data, test_size=val_size_adjusted, random_state=42)
    return train_data, val_data, test_data

# åŠ è½½ JSON æ–‡ä»¶
def load_json(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def parse_llm_output(raw):
    try:
        true_score = float(raw.split("true:")[1].split(",")[0].strip())
        false_score = float(raw.split("false:")[1].split("\n")[0].split("false:")[1].strip())
        rationale = raw.split("Rationale:")[-1].split("Multimodal Consistency Score:")[0].strip()

        if "Multimodal Consistency Score:" in raw:
            mcs = float(raw.split("Multimodal Consistency Score:")[1].split("\n")[0].strip())
        else:
            mcs = None  # è¡¨ç¤ºæ— å¤šæ¨¡æ€ä¿¡æ¯

        return [true_score, false_score], rationale, mcs
    except:
        return [0.5, 0.5], "", None

# å›¾åƒå‘é‡æå–
def extract_image_vector(image_paths, image_root):
    features = []
    if not image_paths:  # å¦‚æœæ²¡æœ‰å›¾ç‰‡è·¯å¾„
        return torch.zeros(2048).to(device)  # è¿”å›ä¸€ä¸ªé›¶å‘é‡ï¼Œé•¿åº¦ä¸º 2048ï¼ˆä¸ ResNet è¾“å‡ºä¸€è‡´ï¼‰

    for img in image_paths:
        try:
            img_path = os.path.join(image_root, img)
            image = Image.open(img_path).convert("RGB")
            image = image_transform(image).unsqueeze(0).to(device)
            with torch.no_grad():
                vec = resnet(image)
            features.append(vec)
        except Exception as e:
            # print(f"Error processing image {img}: {e}")
            continue
    if features:
        return torch.mean(torch.stack(features), dim=0).squeeze(0)
    else:
        return torch.zeros(2048).to(device)  # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå›¾ç‰‡ï¼Œè¿”å›ä¸€ä¸ªé›¶å‘é‡


# è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
def compute_cosine_similarity(text_vec, image_vec):
    # Ensure that the vectors are 2D arrays (batch_size, features)
    text_vec = text_vec.unsqueeze(0) if text_vec.dim() == 1 else text_vec  # Convert to 2D if 1D
    image_vec = image_vec.unsqueeze(0) if image_vec.dim() == 1 else image_vec  # Convert to 2D if 1D

    # Now compute cosine similarity
    cosine_sim = cosine_similarity(text_vec.cpu().detach().numpy(), image_vec.cpu().detach().numpy())
    return torch.tensor(cosine_sim[0][0]).to(device)


class RumorDataset(Dataset):
    def __init__(self, data_list, llm_data, stance_data, fact_data, tokenizer, image_root, max_length=256):
        self.data = []
        self.tokenizer = tokenizer
        self.max_length = max_length

        for item in data_list:
            item_id = str(item["event_id"])
            content = item["content"]
            label = int(item["label"])
            images = item.get("image", [])

            # LLMæ•°æ®
            llm_raw = llm_data.get(item_id, "")
            llm_score, rationale_text, mcs_score = parse_llm_output(llm_raw)

            # stance score
            stance_obj = stance_data.get("event_id: " + item_id, {})
            sent_labels = stance_obj.get("rationale_sentences", [])
            global_label = stance_obj.get("rationale_overall_prediction", {}).get("predicted_label", "")
            match_count = sum(1 for s in sent_labels if s.get("predicted_label") == global_label)
            stance_score = match_count / max(len(sent_labels), 1)

            # fact score
            fact_score = fact_data.get(item_id, {}).get("Wikipedia Score (Rationale)", 0.5)
            try:
                fact_score = float(fact_score)
            except:
                fact_score = 0.5

            # ç¼–ç æ–‡æœ¬
            claim_enc = tokenizer(content, padding="max_length", truncation=True, max_length=max_length, return_tensors="pt")
            rationale_enc = tokenizer(rationale_text, padding="max_length", truncation=True, max_length=max_length, return_tensors="pt")

            # å›¾åƒå‘é‡
            image_vec = extract_image_vector(images, image_root)

            self.data.append({
                "claim_ids": claim_enc["input_ids"].squeeze(0),
                "claim_mask": claim_enc["attention_mask"].squeeze(0),
                "rationale_ids": rationale_enc["input_ids"].squeeze(0),
                "rationale_mask": rationale_enc["attention_mask"].squeeze(0),
                "image_vec": image_vec,
                "stance_score": torch.tensor([stance_score], dtype=torch.float),
                "fact_score": torch.tensor([fact_score], dtype=torch.float),
                "mcs_score": torch.tensor([mcs_score if mcs_score is not None else -1], dtype=torch.float),  # -1 ä»£è¡¨ç¼ºå¤±
                "llm_score": torch.tensor(llm_score, dtype=torch.float),
                "label": torch.tensor(label, dtype=torch.long)
            })

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]


class FusionModel(nn.Module):
    def __init__(self, bert_path, d_s=32, hidden_dim=256, num_labels=2):
        super(FusionModel, self).__init__()
        self.bert = BertModel.from_pretrained(bert_path)
        hidden_size = self.bert.config.hidden_size  # ä¸€èˆ¬ä¸º 768

        # ä¸‰ä¸ªæ‰“åˆ†è½¬å‘é‡
        self.proj_stance = nn.Sequential(nn.Linear(1, d_s), nn.ReLU())
        self.proj_fact = nn.Sequential(nn.Linear(1, d_s), nn.ReLU())
        self.proj_mcs = nn.Sequential(nn.Linear(1, d_s), nn.ReLU())

        # å›¾åƒå‘é‡æ˜ å°„åˆ° BERT å‘é‡ç©ºé—´ï¼ˆå°†å›¾åƒç‰¹å¾ä» 2048 æ˜ å°„åˆ° 768ï¼‰
        self.image_proj = nn.Linear(2048, hidden_size)

        # èåˆæ‰€æœ‰å‘é‡
        self.fusion = nn.Sequential(
            nn.Linear(hidden_size * 3 + 3 * d_s, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )

        self.classifier = nn.Linear(hidden_dim, num_labels)

    def forward(self, claim_ids, claim_mask, rationale_ids, rationale_mask,
                image_vec, stance_score, fact_score, mcs_score, llm_confidence, return_vectors=False):
        # è·å–æ–‡æœ¬å‘é‡ï¼ˆclaim_vecï¼‰
        claim_vec = self.bert(claim_ids, attention_mask=claim_mask).pooler_output

        # è·å–æ¨ç†å‘é‡ï¼ˆrationale_vecï¼‰
        rationale_vec = self.bert(rationale_ids, attention_mask=rationale_mask).pooler_output

        # å¦‚æœæ²¡æœ‰å›¾åƒä¿¡æ¯ï¼Œä½¿ç”¨ä¸€ä¸ªé›¶å‘é‡
        if image_vec.size(0) == 0:
            image_embed = torch.zeros_like(claim_vec)  # å°† image_embed è®¾ç½®ä¸ºä¸æ–‡æœ¬å‘é‡ç›¸åŒç»´åº¦çš„é›¶å‘é‡
        else:
            # å›¾åƒåµŒå…¥ï¼ˆimage_vecï¼‰
            image_embed = self.image_proj(image_vec)

        # èåˆé¢å¤–çš„æ‰“åˆ†ä¿¡æ¯
        stance_embed = self.proj_stance(stance_score)
        fact_embed = self.proj_fact(fact_score)

        # è®¡ç®—å›¾åƒ-æ–‡æœ¬çš„ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºmcs_cos
        mcs_cos = compute_cosine_similarity(claim_vec, image_embed)

        # å½’ä¸€åŒ–å¤„ç†SLMçš„mcs_cos
        mcs_cos = (mcs_cos - mcs_cos.min()) / (mcs_cos.max() - mcs_cos.min() + 1e-8)  # é˜²æ­¢é™¤é›¶é”™è¯¯

        mcs_score = torch.where(mcs_score < 0, torch.zeros_like(mcs_score), mcs_score)

        # ä½¿ç”¨LLMçš„ç½®ä¿¡åº¦åŠ æƒLLMçš„MCSå’ŒSLMçš„MCS
        final_mcs = (llm_confidence * mcs_score + (1 - llm_confidence) * mcs_cos) / 2

        # å¯¹äºç¼ºå¤±çš„ MCS ä¿¡æ¯ï¼Œè®¾ä¸ºé›¶
        mcs_embed = self.proj_mcs(final_mcs)

        # å°†æ‰€æœ‰çš„å‘é‡è¿æ¥èµ·æ¥
        fusion_input = torch.cat([claim_vec, rationale_vec, image_embed, stance_embed, fact_embed, mcs_embed], dim=-1)
        fused_rep = self.fusion(fusion_input)

        # è·å–æœ€ç»ˆçš„åˆ†ç±»ç»“æœ
        logits = self.classifier(fused_rep)

        return logits, mcs_cos


def final_decision(llm_score, slm_logits, beta_llm, beta_slm, mcs_cos, MCS, mcs_threshold=0.3):


    slm_probs = F.softmax(slm_logits, dim=-1)
    llm_pred = torch.argmax(llm_score, dim=-1)
    slm_pred = torch.argmax(slm_probs, dim=-1)

    final_preds = []

    for i in range(len(llm_pred)):
        # å¦‚æœLLMå’ŒSLMçš„é¢„æµ‹ä¸€è‡´ï¼Œç›´æ¥è¾“å‡º
        if llm_pred[i] == slm_pred[i]:
            final_preds.append(llm_pred[i])
        else:
            # å¦‚æœæ²¡æœ‰å›¾ç‰‡ä¿¡æ¯ï¼ˆmcs_cosä¸å­˜åœ¨æˆ–è€…éå¸¸å°ï¼‰
            if MCS[i] == -1:
                # å½“æ²¡æœ‰å›¾ç‰‡ä¿¡æ¯æ—¶ï¼Œç›´æ¥é€‰æ‹©ç½®ä¿¡åº¦æ›´é«˜çš„æ¨¡å‹
                conf_llm = llm_score[i][llm_pred[i]] * beta_llm
                conf_slm = slm_probs[i][slm_pred[i]] * beta_slm
                final_preds.append(slm_pred[i] if conf_slm >= conf_llm else llm_pred[i])
            else:
                # ä½¿ç”¨MCSæ¥å†³å®šï¼ˆæœ‰å›¾ç‰‡ä¿¡æ¯æ—¶ï¼‰
                if mcs_cos[i] < mcs_threshold and MCS[i] < mcs_threshold:
                    final_preds.append(torch.tensor(1))  # è¾“å‡ºä¸º False
                else:
                    conf_llm = llm_score[i][llm_pred[i]] * beta_llm
                    conf_slm = slm_probs[i][slm_pred[i]] * beta_slm
                    final_preds.append(slm_pred[i] if conf_slm >= conf_llm else llm_pred[i])

    return torch.stack(final_preds)


def compute_confidence(model_probs, true_labels, alpha=1.0):
    """
    æ ¹æ® NLL æŸå¤±è®¡ç®—æ¨¡å‹å…¨å±€ç½®ä¿¡åº¦
    C = exp(-Î± Ã— NLL)ï¼Œè¿”å›æ ‡é‡
    """
    nll = 0.0
    total = 0
    for prob, label in zip(model_probs, true_labels):
        nll -= torch.log(prob[label] + 1e-8)
        total += 1


    nll = nll / total
    print(f"SLM Confidence: {torch.exp(-alpha * nll):.4f}")
    return torch.exp(-alpha * nll)


def evaluate_with_fusion(model, loader, beta_llm, beta_slm, mcs_thresh=0.3, name="Eval"):
    model.eval()
    all_preds, all_labels = [], []
    slm_probs, gold_labels, mcs_scores = [], [], []

    with torch.no_grad():
        for batch in loader:
            inputs = {k: v.to(device) for k, v in batch.items() if k not in ["label", "llm_score"]}
            labels = batch["label"].to(device)
            llm_score = batch["llm_score"].to(device)
            MCS = batch["mcs_score"].squeeze(-1).to(device)

            logits, mcs_cos = model(**inputs, llm_confidence=beta_llm)
            probs = F.softmax(logits, dim=-1)

            pred = final_decision(llm_score, logits, beta_llm, beta_slm, mcs_cos, MCS, mcs_thresh)

            all_preds.extend(pred.cpu().tolist())
            all_labels.extend(labels.cpu().tolist())

            slm_probs.extend(probs.cpu())
            gold_labels.extend(labels.cpu())
            mcs_scores.extend(MCS.cpu())

    print(f"\nğŸ“Š {name} Evaluation:")
    # è¾“å‡ºæ··æ·†çŸ©é˜µ
    print("Confusion Matrix:")
    print(confusion_matrix(all_labels, all_preds))

    # è®¡ç®—å¹¶è¾“å‡ºæ€»ä½“å‡†ç¡®ç‡
    accuracy = accuracy_score(all_labels, all_preds)
    print(f"Overall Accuracy: {accuracy:.4f}")
    return slm_probs, gold_labels


def main():
    image_root = "../Datasets/Weibo/images/"
    tokenizer = BertTokenizer.from_pretrained("../Model/bert-base-uncased")
    raw_data_raw = load_json("../Datasets/Weibo/weibo.json")
    raw_data = [{"event_id": k, **v} for k, v in raw_data_raw.items()]

    llm_data = load_json("../Inference/Weibo/No_rebuttal/weibo.json")
    stance_data = load_json("SLM/mutil_model/Weibo/Check_No_rebuttal/Stance/rebuttal_weibo.json")
    fact_data = load_json("SLM/mutil_model/Weibo/Check_No_rebuttal/Fact/rebuttal_weibo.json")


    beta_llm = compute_llm_confidence(llm_data, raw_data_raw, alpha=1.0)

    train_list, val_list, test_list = split_dataset(raw_data)

    train_dataset = RumorDataset(train_list, llm_data, stance_data, fact_data, tokenizer, image_root)
    val_dataset = RumorDataset(val_list, llm_data, stance_data, fact_data, tokenizer, image_root)
    test_dataset = RumorDataset(test_list, llm_data, stance_data, fact_data, tokenizer, image_root)

    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=8)
    test_loader = DataLoader(test_dataset, batch_size=8)

    model = FusionModel("../Model/bert-base-uncased").to(device)
    optimizer = AdamW(model.parameters(), lr=2e-5)

    # è®­ç»ƒï¼ˆæ¯4ä¸ªepochåè¿›è¡Œä¸€æ¬¡éªŒè¯ï¼‰
    num_epochs = 20  # å‡è®¾ä½ æƒ³è®­ç»ƒ20ä¸ªepoch
    eval_frequency = 1  # æ¯4ä¸ªepochè¯„ä¼°ä¸€æ¬¡éªŒè¯é›†

    for epoch in range(1, num_epochs + 1):
        model.train()
        total_loss = 0
        slm_probs = []  # ç”¨äºå­˜å‚¨SLMçš„è¾“å‡ºprobabilities
        gold_labels = []  # ç”¨äºå­˜å‚¨çœŸå®æ ‡ç­¾

        for batch in tqdm(train_loader, desc=f"Epoch {epoch}"):
            inputs = {k: v.to(device) for k, v in batch.items() if k not in ["label", "llm_score"]}
            labels = batch["label"].to(device)

            logits , mcs = model(**inputs, llm_confidence=beta_llm)
            loss = nn.CrossEntropyLoss()(logits, labels)

            # è·å–SLMçš„è¾“å‡ºæ¦‚ç‡
            probs = F.softmax(logits, dim=-1)
            slm_probs.extend(probs.cpu())  # ç”¨äºè®¡ç®—SLMç½®ä¿¡åº¦
            gold_labels.extend(labels.cpu())  # ç”¨äºè®¡ç®—SLMç½®ä¿¡åº¦

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        # æ¯è½®è®­ç»ƒç»“æŸåè®¡ç®—SLMç½®ä¿¡åº¦
        beta_slm_train = compute_confidence(slm_probs, gold_labels)
        print(f"âœ… Epoch {epoch} Training Loss: {total_loss:.4f}")
        print(f"âœ… LLM Global Confidence (Î²1): {beta_llm:.4f}")
        print(f"âœ… Epoch {epoch} SLM Global Confidence (Î²2): {beta_slm_train:.4f}")

        # æ¯4ä¸ªepochè¯„ä¼°ä¸€æ¬¡éªŒè¯é›†
        if epoch % eval_frequency == 0:
            print(f"âœ… Evaluating on Validation set after Epoch {epoch}")
            slm_probs_val, gold_val = evaluate_with_fusion(model, val_loader, beta_llm, beta_slm_train, name="Validation")
            beta_llm = beta_llm  # LLMç½®ä¿¡åº¦å…¨å±€å¸¸é‡
            beta_slm_val = compute_confidence(slm_probs_val, gold_val)  # è®¡ç®—éªŒè¯é›†ä¸Šçš„SLMç½®ä¿¡åº¦
            print(f"âœ… LLM Global Confidence (Î²1): {beta_llm:.4f}")
            print(f"âœ… SLM Global Confidence (Î²2): {beta_slm_val:.4f}")

            # æµ‹è¯•é›†èåˆè¯„ä¼°
            evaluate_with_fusion(model, test_loader, beta_llm, beta_slm_val, name="Test")

if __name__ == "__main__":
    main()

