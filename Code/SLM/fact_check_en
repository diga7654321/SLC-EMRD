import wikipedia
import json
import time
import os
import re
import spacy
from sentence_transformers import SentenceTransformer, util
from sklearn.feature_extraction.text import TfidfVectorizer
import itertools
import requests

# ğŸ”§ **ä»£ç†è®¾ç½®ï¼ˆå¦‚æœéœ€è¦ï¼‰**
os.environ["http_proxy"] = "http://127.0.0.1:7897"
os.environ["https_proxy"] = "http://127.0.0.1:7897"

# ğŸ”§ **è®¾ç½® Wikipedia è¯­è¨€**
wikipedia.set_lang("en")

# ğŸ”§ **åŠ è½½ spaCy è¯­è¨€æ¨¡å‹**
nlp = spacy.load("en_core_web_sm")

# ğŸ”§ **åŠ è½½ BERT è¯­ä¹‰ç›¸ä¼¼åº¦æ¨¡å‹**
semantic_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# ğŸ“‚ **è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶è·¯å¾„**
LLM_DATA_FILE = "../../../../../../Inference/Pheme/No_rebuttal/pheme.json"
FACT_CHECK_RESULTS = "rebuttal_pheme.json"


# ğŸ“Œ **è¯»å– LLM ç”Ÿæˆçš„æ•°æ®**
def load_llm_data(file_path):
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        return {}


# ğŸ“Œ **è¯»å–å·²å¤„ç†çš„ç»“æœï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰**
def load_existing_results(file_path):
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return {}


# ğŸ“Œ **æå–å¤šä¸ªæŸ¥è¯¢å…³é”®è¯**
def extract_query_terms(text, top_n=3):
    """
    1. ä½¿ç”¨ spaCy æå– **å®ä½“**ï¼ˆå¦‚äººåã€ç»„ç»‡ã€åœ°åï¼‰ã€‚
    2. å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚å®ä½“ï¼Œåˆ™ç”¨ **TF-IDF** æå–æœ€é‡è¦çš„ **å‰ top_n ä¸ªå…³é”®è¯**ã€‚
    """
    doc = nlp(text)

    # 1ï¸âƒ£ å°è¯•æå–å®ä½“
    entities = [ent.text for ent in doc.ents if ent.label_ in ["PERSON", "ORG", "GPE"]]

    if len(entities) >= top_n:
        return entities[:top_n]  # ç›´æ¥è¿”å›å‰ top_n ä¸ªå®ä½“

    # 2ï¸âƒ£ å¦‚æœå®ä½“ä¸è¶³ï¼Œä½¿ç”¨ TF-IDF æå–å…³é”®å•è¯
    words = [token.text.lower() for token in doc if token.is_alpha and not token.is_stop]
    if len(words) < top_n:  # è¯æ•°ä¸è¶³ï¼Œè¿”å›æ‰€æœ‰å•è¯
        return words

    vectorizer = TfidfVectorizer(max_features=top_n, stop_words='english')
    X = vectorizer.fit_transform([" ".join(words)])  # è®¡ç®— TF-IDF
    tfidf_scores = dict(zip(vectorizer.get_feature_names_out(), X.toarray().flatten()))

    # é€‰æ‹© TF-IDF æœ€é«˜çš„å•è¯
    top_keywords = sorted(tfidf_scores, key=tfidf_scores.get, reverse=True)[:top_n]

    return list(set(entities + top_keywords))  # å®ä½“ + TF-IDF å…³é”®è¯


# ğŸ“Œ **æå–æ™®é€šç†ç”±å’Œæ¨¡æ€ä¸€è‡´æ€§ç†ç”±**
def extract_reasons(llm_text):
    """
    ä» LLM æ–‡æœ¬ä¸­æå–æ™®é€šç†ç”±å’Œæ¨¡æ€ä¸€è‡´æ€§ç†ç”±
    """
    # æå–æ™®é€šç†ç”±
    rationale_match = re.search(r"Rationale:\s*(.*?)(?=\nMultimodal Consistency Score:|$)", llm_text, re.DOTALL)
    rationale = rationale_match.group(1).strip() if rationale_match else "No rationale found"

    # æå–æ¨¡æ€ä¸€è‡´æ€§ç†ç”±
    multimodal_rationale_match = re.search(r"Multimodal Consistency Rationale:\s*(.*?)(?=\n|$)", llm_text, re.DOTALL)
    multimodal_rationale = multimodal_rationale_match.group(
        1).strip() if multimodal_rationale_match else "No multimodal rationale found"

    return rationale, multimodal_rationale


# ğŸ“Œ **Wikipedia æŸ¥è¯¢**
def search_wikipedia(query_terms):
    """
    1ï¸âƒ£ å…ˆç”¨ `wikipedia.search()` è·å–æœ€ä½³é¡µé¢
    2ï¸âƒ£ å¤„ç† `DisambiguationError`ï¼ˆæ­§ä¹‰é”™è¯¯ï¼‰ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ç›¸å…³çš„é¡µé¢
    3ï¸âƒ£ å¦‚æœ `wikipedia.summary()` å¤±è´¥ï¼Œåˆ™ç”¨ Wikipedia API å…œåº•
    """
    search_results = []

    # 1ï¸âƒ£ **å…ˆå°è¯•ä¸åŒç»„åˆçš„æœç´¢**
    for i in range(len(query_terms), 0, -1):
        for subset in itertools.combinations(query_terms, i):
            query = " ".join(subset)
            print(f"ğŸ” å°è¯• Wikipedia æŸ¥è¯¢: {query}")

            try:
                search_results = wikipedia.search(query)
                if search_results:
                    break  # æ‰¾åˆ°åŒ¹é…é¡¹å°±åœæ­¢
            except wikipedia.exceptions.DisambiguationError as e:
                print(f"âš ï¸ å…³é”®è¯ `{query}` å…·æœ‰å¤šä¸ªè§£é‡Šï¼Œè‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€ä¸ªå¯èƒ½é¡µé¢: {e.options[:3]}")
                search_results = e.options[:3]  # é€‰æ‹©å‰ 3 ä¸ªå¯èƒ½é¡µé¢
                break
            except wikipedia.exceptions.PageError:
                continue

        if search_results:
            break

    # 2ï¸âƒ£ **å¦‚æœæ²¡æœ‰ç»“æœï¼Œä½¿ç”¨ Wikipedia API å…œåº•**
    if not search_results:
        api_url = f"https://en.wikipedia.org/w/api.php?action=query&format=json&list=search&srsearch={query}"
        try:
            response = requests.get(api_url, timeout=5).json()
            search_results = [item["title"] for item in response["query"]["search"]][:3]
        except Exception as e:
            print(f"âŒ Wikipedia API æŸ¥è¯¢å¤±è´¥: {e}")
            return None

    # 3ï¸âƒ£ **å°è¯•è·å– Wikipedia é¡µé¢æ‘˜è¦**
    for page_title in search_results:
        try:
            print(f"ğŸ“Œ é€‰æ‹© Wikipedia é¡µé¢: {page_title}")
            return wikipedia.summary(page_title, sentences=5, auto_suggest=False)
        except wikipedia.exceptions.DisambiguationError as e:
            print(f"âš ï¸ `{page_title}` ä»ç„¶å­˜åœ¨æ­§ä¹‰ï¼Œå°è¯• `{e.options[0]}`")
            try:
                return wikipedia.summary(e.options[0], sentences=5, auto_suggest=False)
            except wikipedia.exceptions.PageError:
                continue
        except wikipedia.exceptions.PageError:
            print(f"âŒ é¡µé¢ `{page_title}` ä¸å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
            continue
    return None


# ğŸ“Œ **BERT è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—**
def compute_semantic_similarity(text1, text2):
    """
    è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆèŒƒå›´ 0-1ï¼‰ã€‚
    """
    embeddings1 = semantic_model.encode(text1, convert_to_tensor=True)
    embeddings2 = semantic_model.encode(text2, convert_to_tensor=True)
    similarity_score = util.pytorch_cos_sim(embeddings1, embeddings2).item()
    return similarity_score


# ğŸ“Œ **äº‹å®æ ¸æŸ¥è¯„åˆ†**
def fact_check(llm_rationale, wikipedia_text):
    """
    è®¡ç®— LLM è§£é‡Šä¸ Wikipedia è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œå¹¶ç»™å‡ºåŒ¹é…åˆ†æ•°ã€‚
    """
    if not wikipedia_text:
        return 0, "No Evidence Found"

    similarity_score = compute_semantic_similarity(llm_rationale, wikipedia_text)

    if similarity_score >= 0.5:
        label = "True"
    elif 0.25 <= similarity_score < 0.5:
        label = "Partial"
    else:
        label = "False"

    return similarity_score, label


# ğŸ“Œ **ä¿å­˜æ ¸æŸ¥ç»“æœ**
def save_fact_check_results(results):
    with open(FACT_CHECK_RESULTS, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=4, ensure_ascii=False)


# ğŸ“Œ **è¿è¡Œäº‹å®æ ¸æŸ¥**
def run_fact_check():
    llm_data = load_llm_data(LLM_DATA_FILE)
    fact_check_results = load_existing_results(FACT_CHECK_RESULTS)

    total_items = len(llm_data)
    processed_count = len(fact_check_results)

    print(f"ğŸ” æ€»å…±æœ‰ {total_items} æ¡æ•°æ®ï¼Œå·²å¤„ç† {processed_count} æ¡ï¼Œå‰©ä½™ {total_items - processed_count} æ¡ã€‚\n")

    for idx, (item_id, llm_text) in enumerate(llm_data.items(), start=1):
        if item_id in fact_check_results:
            continue

        # ğŸ” æå–æ™®é€šç†ç”±å’Œæ¨¡æ€ä¸€è‡´æ€§ç†ç”±
        rationale, multimodal_rationale = extract_reasons(llm_text)

        # ---- å¤„ç†æ™®é€šç†ç”± ----
        query_terms_rationale = extract_query_terms(rationale, top_n=3)
        wikipedia_info_rationale = search_wikipedia(query_terms_rationale)
        match_score_rationale, label_rationale = fact_check(rationale, wikipedia_info_rationale)

        # ---- å¤„ç†æ¨¡æ€ä¸€è‡´æ€§ç†ç”±ï¼ˆå¦‚æœ‰ï¼‰----
        if multimodal_rationale == "No multimodal rationale found" or not multimodal_rationale.strip():
            print(f"âš ï¸ æœªæ‰¾åˆ°æ¨¡æ€ä¸€è‡´æ€§ç†ç”±ï¼Œè·³è¿‡: item_id={item_id}")
            query_terms_multimodal_rationale = []
            wikipedia_info_multimodal_rationale = ""
            match_score_multimodal_rationale = ""
            label_multimodal_rationale = "No Evidence Found"
        else:
            query_terms_multimodal_rationale = extract_query_terms(multimodal_rationale, top_n=3)
            wikipedia_info_multimodal_rationale = search_wikipedia(query_terms_multimodal_rationale)
            match_score_multimodal_rationale, label_multimodal_rationale = fact_check(
                multimodal_rationale, wikipedia_info_multimodal_rationale
            )

        # âœ… ä¿å­˜å¤„ç†ç»“æœ
        fact_check_results[item_id] = {
            "LLM Rationale": rationale,
            "Multimodal Consistency Rationale": multimodal_rationale,
            "Query Terms (Rationale)": query_terms_rationale,
            "Query Terms (Multimodal Rationale)": query_terms_multimodal_rationale,
            "Wikipedia Score (Rationale)": match_score_rationale,
            "Wikipedia Label (Rationale)": label_rationale,
            "Wikipedia Evidence (Rationale)": wikipedia_info_rationale if wikipedia_info_rationale else "No Evidence",
            "Wikipedia Score (Multimodal Rationale)": match_score_multimodal_rationale,
            "Wikipedia Label (Multimodal Rationale)": label_multimodal_rationale,
            "Wikipedia Evidence (Multimodal Rationale)": wikipedia_info_multimodal_rationale if wikipedia_info_multimodal_rationale else "No Evidence"
        }

        save_fact_check_results(fact_check_results)

        print(f"âœ… å®Œæˆ {item_id}: æ™®é€šç†ç”±å¾—åˆ† {match_score_rationale} â†’ {label_rationale}")
        print(f"âœ… å®Œæˆ {item_id}: æ¨¡æ€ç†ç”±å¾—åˆ† {match_score_multimodal_rationale} â†’ {label_multimodal_rationale}")

        time.sleep(5)  # é˜²æ­¢è¯·æ±‚è¿‡å¿«è¢«å°

    print(f"\nğŸ‰ æ‰€æœ‰æ•°æ®å¤„ç†å®Œæ¯•ï¼Œç»“æœå·²ä¿å­˜è‡³ {FACT_CHECK_RESULTS}")


# ğŸš€ **è¿è¡Œ**
if __name__ == "__main__":
    run_fact_check()
