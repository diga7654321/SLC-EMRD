from openai import OpenAI
import json
import time
import re

# ğŸ”§ **API è®¾ç½®**
OPENAI_KEY = ""
OPENAI_BASE = ""
client = OpenAI(api_key=OPENAI_KEY, base_url=OPENAI_BASE)

# ğŸ“‚ **æ–‡ä»¶è·¯å¾„**
news_file = "../../../Datasets/RAWFC/train.json"#æ–°é—»
inference_file = "../../../Inference/RAWFC/No_rebuttal/train.json"#ç«‹åœºæ£€æµ‹åLLMçš„æ¨ç†
rebuttal_file = "../../SLM/only_model/Check_No_rebuttal/Fact/rebuttal_train.json"  #äº‹å®æ ¸æŸ¥æ¨¡å‹çš„æ£€æµ‹ç»“æœ
output_file = "../../../Inference/RAWFC/Rebuttal_fact/Rebuttal_train.json"  # ä¿å­˜é‡æ–°åˆ†æåä»¥åŠæœªé‡æ–°åˆ†æçš„æ•°æ®
modified_output_file = "../../../Inference/RAWFC/Rebuttal_fact/rebuttal_train_modified.json"  # è®°å½•ä¿®æ”¹çš„æ¡ç›®


def generate_reanalysis_prompt(news_sample, previous_llm_analysis, fact_check_status, evidence):
    return f"""You are a professional **rumor detection expert**. Below is a news sample, along with your previous AI-generated analysis. Consideration of the current situation, analysis of new developments.

---

### ğŸ” News Sample:
{news_sample}


### Your Previous LLM Analysis:
{previous_llm_analysis}


###  Fact-Checking Feedback:
- Evidence from External Sources:
{evidence}

---

##  Your Task:

###  Evaluation Guidelines:
1. Logical Consistency â†’ Does the claim make sense when examined step by step?
2. Factual Accuracy â†’ Does the claim align with verified facts and external evidence?
3. Common Sense Validation â†’ Does the claim conform to widely accepted knowledge?

---

##  Scoring Criteria (Total = 1.0)
- True â†’ Completely factual, no major issues.
- Half â†’ Partially true, but contains misleading elements.
- False â†’ Completely false, contradicts verified facts.

###  Expected Output Format
Your response **must strictly follow this format** as a plain text output:

Score: true: 0.X, half: 0.X, false: 0.X

Rationale: Provide a clear, fact-based explanation...

"""



# ğŸ“‚ **è¯»å– JSON æ–‡ä»¶**
def load_json_file(file_path):
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            return json.load(file)
    except (json.JSONDecodeError, FileNotFoundError):
        print(f"âš ï¸ Warning: Could not load {file_path}, initializing an empty dataset.")
        return {} if file_path.endswith(".json") else []


# ğŸ“Œ **åŠ è½½æ•°æ®**
news_data = load_json_file(news_file)  # åŸå§‹æ–°é—»æ•°æ®
inference_data = load_json_file(inference_file)  # ç¬¬ä¸€æ¬¡LLMåˆ†æç»“æœ
rebuttal_data = load_json_file(rebuttal_file)  # éœ€è¦é‡æ–°åˆ†æçš„ event_id
modified_data = {}  # è®°å½•è¢«ä¿®æ”¹çš„ event_id

# ğŸ¯ **è§£æ event_idï¼Œç¡®ä¿åŒ¹é…æ–°é—»æ•°æ®**
news_samples = {
    str(item["event_id"]): f"Claim: {item['claim']}\nExplain: {item.get('explain', 'No_rebuttal explanation available')}"
    for item in news_data
}


# ğŸ” **è§£æ LLM å“åº”**
def parse_llm_response(response):
    """
    è§£æ LLM è¿”å›çš„æ–‡æœ¬ï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®ï¼Œå¹¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²å­˜å‚¨æ ¼å¼
    """
    match = re.search(
        r"Score:\s*true:\s*([\d.]+),\s*half:\s*([\d.]+),\s*false:\s*([\d.]+)\n\nRationale:\s*(.*)",
        response,
        re.DOTALL,
    )
    if match:
        true_score = float(match.group(1))
        half_score = float(match.group(2))
        false_score = float(match.group(3))
        rationale = match.group(4).strip()

        # ç¡®ä¿æ€»åˆ†ä¸º 1.0
        if abs((true_score + half_score + false_score) - 1.0) < 1e-6:
            return f"Score: true: {true_score}, half: {half_score}, false: {false_score}\n\nRationale: {rationale}"

    return None


# ğŸ”„ **éå† `rebuttal_file` æ‰¾å‡ºéœ€è¦é‡æ–°åˆ†æçš„ event_id**
for event_id_key, event_data in rebuttal_data.items():
    event_id = str(event_id_key).replace("event_id: ", "")  # ç»Ÿä¸€ event_id æ ¼å¼

    # ğŸ· **è·å– `all_sentences_prediction` æ ‡ç­¾**
    all_LLMrationale = event_data.get("LLM Rationale", {})
    all_evidence = event_data.get("Wikipedia Evidence", {})
    all_labels = event_data.get("Wikipedia Label", "unknown")

    # ğŸ›‘ **å¦‚æœè¶…è¿‡ 50% å¥å­çš„æ ‡ç­¾ä¸ `all_sentences_label` ä¸åŒï¼Œåˆ™é‡æ–°åˆ†æ**
    if all_labels == "False":
        # ğŸš€ **è·å–æ–°é—»æ ·æœ¬**
        news_sample = news_samples.get(event_id, "Unknown News Sample")

        context_messages = [
            {"role": "system",
                "content": generate_reanalysis_prompt(news_sample, all_LLMrationale, all_labels, all_evidence)}
        ]

        retry_count = 0
        max_retries = 3

        while retry_count < max_retries:
            try:
                # ğŸš€ **è°ƒç”¨ LLM é‡æ–°è¯„ä¼°**
                response = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=context_messages
                )
                result = response.choices[0].message.content.strip()

                # ğŸš€ **è§£æ LLM è¿”å›ç»“æœ**
                parsed_result = parse_llm_response(result)

                if parsed_result:
                    inference_data[event_id] = parsed_result  # **å­˜å‚¨å­—ç¬¦ä¸²æ ¼å¼**
                    modified_data[event_id] = parsed_result  # è®°å½•è¢«ä¿®æ”¹çš„æ•°æ®
                    print(f"âœ… Event {event_id} updated successfully.")
                    break  # ç»“æŸé‡è¯•
                else:
                    print(f"âŒ Invalid response format for event_id: {event_id}. Retrying...")

            except Exception as e:
                print(f"âŒ API request error for event_id: {event_id}: {e}. Retrying...")

            retry_count += 1
            time.sleep(5 + retry_count * 2)  # æŒ‡æ•°é€€é¿
# ğŸ’¾ **ä¿å­˜æ•°æ®**
with open(output_file, "w", encoding="utf-8") as file:
    json.dump(inference_data, file, ensure_ascii=False, indent=4)

with open(modified_output_file, "w", encoding="utf-8") as file:
    json.dump(modified_data, file, ensure_ascii=False, indent=4)

print(f"âœ… Updated data saved to {output_file}")
print(f"âœ… Modified entries saved to {modified_output_file}")
